--- 
title: "Causality and Multiple Regression Supplement"
author: "Krista Watts and Kevin Cummiskey"
date: "`r Sys.Date()`"
site: bookdown::bookdown_site
documentclass: book
bibliography: [book.bib, packages.bib]
biblio-style: apalike
link-citations: yes
description: "This is a supplement for MA206 Probability and Statistics, West Point, NY."
---

# Preface {-}

This etext is an introduction to causality in statistics and multivariable methods for students in their first college course in statistics.  Traditionally, except for a terse warning from instructors that "correlation does not imply causation", students only encounter causality in statistics if they take graduate-level courses in certain disciplines (economics, epidemiology, etc.).  However, given the ubiquity of data-driven arguments in modern society, a deeper understanding of drawing cause-and-effect conclusions from data is a core competency of college graduates and deserves attention in the liberal arts curriculum.  For more on causality in the undergraduate curriculum, see our paper [@cummiskey2020causal] and others on this subject [@horton2015challenges; @kaplan2018teaching; @lubke2020we].  

After introducing concepts in causality, we use an example-based approach to multiple regression emphasizing how the scientific goal of the study impacts modeling decisions and interpretation of results.  Prediction and cause-and-effect are the two main scientific goals of studies using multiple regression. These goals determine how researchers use and interpret models.  However, this distinction is rarely made in introductory courses.  Too often, multiple regression in the introductory course focuses on prediction and much of the "thinking" outsourced to algorithmic searches.  This narrow focus on prediction deprives students of the opportunity to exercise critical judgment and creativity.  On the other hand, cause-and-effect studies require tying subject matter knowledge to developing causal models.  During this process, students have to carefully consider relationships between variables (which is not particularly important in prediction), thus developing multivariable thinking, an important goal of the revised GAISE report [@carver2016guidelines].  Our approach to multiple regression uses the scientific goal to motivate how we think about our data and models.  @hernan2019second argue for this approach in data science education; we believe it is appropriate for introductory statistics courses also.        

This etext is appropriate for a wide variety of introductory statistics courses including both algrebra-based and calculus-based courses.  It assumes students understand basic concepts in data analysis, inference (confidence intervals and statistical tests for means and proportions), and simple linear regression.


```{r include=FALSE}
# automatically create a bib database for R packages
knitr::write_bib(c(
  .packages(), 'bookdown', 'knitr', 'rmarkdown'
), 'packages.bib')
```

<!--chapter:end:index.Rmd-->

# Introduction {#introduction}

## Goals of quantitative research (describe, predict, cause-and-effect)

Researchers and practitioners in nearly every discipline employ quantitative methods to answer important questions and make decisions.  Regardless if it is finance, medicine, psychology, etc., all researchers typically have one of three goals [@cozby2020methods] when they conduct quantitative research:

* **describe**

* **predict** 

* **cause-and-effect**


For example, consider medical researchers investigating a newly discovered variant of the coronavirus.  Initially, they might focus on describe studies.  *How prevalent is the variant in the population? How does the prevalence vary over space and time? What groups of individuals are most vulnerable to the variant? What else is associated with the variant?*   Together, these studies help researchers better understand the variant and identify important variables for refined predict studies.  

Predict studies identify individuals most at risk for the variant or for transmitting it to others. Researchers select potential predictors as inputs to algorithms and statistical methods that transform them into estimates of each individual's risk for the variant.  These methods rely upon associations between the inputs and the variant.  In addition, these methods estimate the error associated with their predictions. Researchers try to ensure error estimates obtained within their study are good estimates of the error when their models are applied to other populations.  While predict studies help us identify who is at risk, they do not tell us the effect if we change something about individuals (quit smoking, lose weight, etc.)  To answer these "what if?" questions, researchers use cause-and-effect studies.

Cause-and-effect studies try to estimate the effect of intervening or changing some aspect of an individual.  These studies are tremendously important in public policy (where they are aptly called *intervention studies*) to justify legislation and regulations to reduce risk.  Unlike prediction studies, which are highly algorithmic and only require researchers to identify appropriate inputs, cause-and-effect studies use subject matter knowledge of the causal structure of the inputs to select appropriate statistical models [@hernan2019second].  For example, to assess the affect of quitting smoking on risk for the variant, we need to understand why some people quit smoking and others do not, as these reasons themselves may be responsible for some changes in risk between the two groups. Unlike prediction studies which rely upon what we *see*, cause-and-effect studies require us to ask what would happen if we *do* something [@pearl2018book].  Only human beings are capable of such counterfactual thinking ("what would have happened if") - even our most sophisticated computer algorithms require human supervision to address cause-and-effect questions.

**Exercises**

1. For the following studies, identify the goal (describe, predict, cause-and-effect) of the study.  Briefly explain your choice:

* *A researcher obtains school records to collect information on the extracurricular activities of students.*

* *A researcher obtains school records to determine the impact of extracurricular activities on grades.*

* *A researcher obtains school records on extracurricular activities to identify students who are at risk to graduate.*

2.  Select an area of interest to you.  Write a sentence describing a study in your area of interest for each of the three goals (describe, predict, cause-and-effect)

## Validity

* internal validity

* external validity

In an area of research, there is often a natural tradeoff between internal and external validity.  Our knowledge comes from both observational studies and experiments (trials) -- rarely does a single study answer every question and the two types of studies used together increase our overall knowledge. Consider researchers investigating the effectiveness of Covid-19 vaccines.  Initial vaccine trials focused on internal validity to understand its effectiveness against the virus and obtain approval for mass distribution.  For example, @polack2020safety randomly assigned 43,548 persons 16 years of age or older to receive two doses of either placebo or the BNT162b2 vaccine (Pfizer).  They observed eight cases of Covid-19 among participants who received BNT162b2 and 162 cases among those who received the placebo.  This was a huge moment in the pandemic! However, the subjects in the study were relatively healthy and had volunteered for the trials.  From this study alone, the external validity of these results is not clear -- will the vaccine have the same effect in the general population?  Given the success of the trials and the need to widely distribute, it would be unethical to withhold the vaccine from large segments of the population to conduct further trials of other populations.  In this case, researchers use observational studies.

@dagan2021bnt162b2 conducted an observational study after the mass vaccination campaign in Israel using the health records of 4.7 million patients enrolled in its largest integrated health care organization.  The researchers matched vaccine recipients to controls on important variables: age, sex, sector, neighborhood of residence, etc.. In this larger sample, they found vaccine effectiveness results consistent with the randomized trial, thus providing evidence of the vaccine's effectiveness in the general public.  The authors specifically address the importance of observational studies:  

> "Although randomized clinical trials [experiments] are considered the “gold standard” for evaluating intervention effects, they have notable limitations of sample size and subgroup analysis, restrictive inclusion criteria, and a highly controlled setting that may not be replicated in a mass vaccine rollout." 

**Exercises**

1. Read the article ["Dozens to be deliberately infected with coronavirus in UK ‘human challenge’ trials"](https://www.nature.com/articles/d41586-020-02821-4) in Nature News. Write a paragraph discussing issues of internal and external validity.


<!--chapter:end:005-introduction.Rmd-->

# Causality {#causality}

The last few decades have seen a revolution in how statisticians view causality.  A hundred years ago, when the statistical methods students learn in introductory courses were developed, causality was considered outside the realm of statistics, except for the case of randomized controlled experiments. In other words, statistics could only answer questions of association, but not of causality.  However, this limited view of statistics was at odds with its usage in every day research.  For example, the greatest public health triumph of the 20th century was the reduction in cigarette smoking, which exploded after World War II with their mass production and marketing.  The statistical evidence of the health effects of smoking comes entirely from observational studies -- there has never been a randomized controlled trial for smoking.  The [American Cancer Society's observational studies](https://www.cancer.org/latest-news/the-study-that-helped-spur-the-us-stop-smoking-movement.html) beginning in the 1950's were huge undertakings and provided compelling evidence of the harmful effects of cigarette smoking [@hammond1954relationship;@hammond1966smoking].   Clearly, statistical theory and practice had diverged in their understandings of causality.



However, in the 1980's and 1990's, researchers in different disciplines began revisiting causality [@greenland1986identifiability; @pearl1993bayesian; @angrist1995identification].  They developed mathematical language for expressing causation, which cannot be uniquely expressed using the traditional language of association. In addition, they showed randomized controlled trials are special cases of more general situations when the researcher has full knowledge of the assignment mechanism.  The *assignment mechanism* is the process in which subjects are assigned to different levels of the treatment.  Lastly, they showed that causal effects could be estimated from observational studies under a wide variety of circumstances when the assignment mechanism is known.  Today, while "correlation does not imply causation" is still useful advice when assessing causal claims in observational studies, statistical theory and practice suggest our assessment of causal claims in observational studies should be richer and more nuanced than this simple rule of thumb. 

## What does it mean for one thing to cause another

We say one variable (called the treatment, exposure, or intervention) *causes* another variable (the outcome or response) if there is a change in the average outcome between subjects when they receive the treatment and the same subjects when they do not receive the treatment.  This definition differs from *association*, which is a change in the average outcome between subjects who received the treatment and different subjects who did not receive the treatment.  Thus, causation is a comparison of observed outcomes and their counterfactuals ("what would have happened if the subject were in the other treatment group").

The central challenge in causal inference is that we cannot observe the outcomes for subjects under both levels of the treatment variable.  In other words, we only get to observe each subject in either the treatment or the control group. For example, when estimating the effect of smoking on long term health outcomes, it is impossible to observe the same subject as a smoker and as a nonsmoker.  However, under certain circumstances, we can obtain good estimates of effects without observing both outcomes for each individual.  The most famous of these is the randomized controlled experiment.

## Randomized controlled experiments

One of the most important scientific discoveries of the early 20th century was the randomized controlled trial (RCT).  In its simplest form, researchers randomly assign subjects to receive the treatment or be in the control group. If they observe a difference in average outcomes between the two groups, then we would say the treatment caused the outcome. *Why does assigning subjects to groups by the simple action of flipping a coin result in such a radical difference in how we interpret the results?* The answer lies in the definition of causation above.  Causation compares the outcomes between the same subjects. When we randomize the treatment, we end up comparing the outcomes between one group of subjects with the treatment and another group of subjects without the treatment who we expect to be very similar.  In fact, when we have large enough sample sizes, it would be very unusual for the two groups to differ much.  We refer to the two groups as *exchangeble*.  In other words, we would expect the control group to have had similar results as the treatment group if they were the treatment group, and vice versa.      

However, an overly restrictive view of causality followed this important discovery.  That is, causality can *only* be shown with RCTs.  This placed a huge limitation on the types of research questions statistics could address. Frequently, RCTs are not ethical, feasible, or desirable.  Imagine enrolling in a study where you could be randomly assigned to be a smoker for the next 20 years.  Towards the end of the 20th century, researchers began taking a more expansive view of causality in observational studies.    

## Observational Studies and Confounding

In observational studies, researchers do not intervene on the assignment of subjects to treatment and control groups. (Note: a common misconception is that observational studies do not have treatment and control groups. This is not true.  It is about how subjects are *assigned* to the treatment groups.) Instead, other factors determine subjects' treatment group assignment.  Confounding occurs when these other factors determining assignment are themselves causes of the outcome.  In other words, the treatment and control groups are different in ways that are important to the outcome. The two groups are not exchangeable.  An observed association between the treatment and outcome could mean (1) the treatment caused the outcome, (2) other factors causing group assignment caused the outcome, or (3) both.  Furthermore, with only information on the treatment and outcome, it is not possible to identify which of the three is the correct explanation.  Confounding is a form of statistical bias -- using the observed association as an estimate of the treatment effect will be systemically off.  Increasing the sample size does not fix bias, you just get a more precise, wrong estimate.

For example, consider an observational study investigating long term health effects of smoking.  In many populations, males are more likely to be smokers. In addition, males have different risks for long term health outcomes than females, regardless of whether they smoke.  If we observe an association between smoking and an outcome without information on sex, we cannot distinguish the effect of smoking from the effect of sex.  

However, researchers in the late 20th century had a key insight.  If we know the assignment mechanism and measure a sufficient set of confounding variables, you can obtain good estimates of treatment effects from observational studies.  This was huge! Observational studies and RCTs are not fundamentally different. Estimating effects requires understanding the assignment mechanism, and RCTs are just a special case with a simple, known assignment mechanism.   Given this insight, researchers became more comfortable making causal claims from observational studies when they have knowledge of the assignment mechanism. *How do we identify a sufficient set of confounding variables?* For that question, we turn to causal diagrams. 


## Causal Diagrams

Causal diagrams are useful tools for depicting the assignment mechanism (also called the causal model). Experts use subject matter knowledge in their field to specify the causal model.  They typically specify the causal model prior to collecting data to identify confounding variables to measure.  Importantly, there is no way to determine the presence of unmeasured confounding using the data.  In addition, using simple heuristics for causal diagrams, they help identify a sufficient set of confounding variables to control for during design and analysis.   

Causal diagrams are directed, acyclic graphs (DAGs) where the nodes are variables. A directed edge (arrow) connecting two nodes indicates the node at the arrow's tail is a cause of the node at the arrow's head.  The graphs are directed because the arrows point in one direction.  They are acyclic because you can never get back to where you started by following arrows.  The convention in many disciplines is to order the variables temporally from left to right -- we adopt that convention here.  

There are three building blocks of causal diagrams.

### Confounding variable

Figure \@ref(fig:confounder) depicts the confounding variable $C$ of the effect of treatment $X$ on outcome $Y$.  We will observe an association between $X$ and $Y$ even if there is no treatment effect. The levels of $X$ differ in terms of $C$ and $C$ itself is a cause of $Y$. Without measuring and controlling for $C$, we cannot distinguish the effect of $X$ on $Y$ from the association through confounding variable $C$.  However, if $C$ is the only confounding variable, controlling for it will result in good estimates of the effect of $X$ on $Y$.  

(backdoor path)

![(#fig:confounder) A confounding variable $C$ on the effect of $X$ on $Y$.](./images/confounder.png)

For example, let's say researchers investigate the effect of a master's degree on adult earnings.  They survey a large number of individuals and record whether they have a master's degree and their earnings. Socioeconomic status is a confounding variable.  Individuals with higher socioeconomic status are more likely to earn master's degrees.  In addition, they are more likely to have higher adult earnings, regardless of whether they have a master's degree.  For this example, let's assume socioeconomic status is the only confounding variable.  If the researchers observe an association between master's degrees and earnings without information on socioeconomic status, it is not possible to determine if the association is due to an effect of master's degrees or the effect of socioeconomic status.   

### Collider

Figure \@ref(fig:collider) depicts treatment $X$ with no effect on outcome $Y$. $X$ and $Y$ are both causes of collider $Z$ (the two incoming arrows *collide*).  The box around $Z$ indicates conditioning upon $Z$ in the analysis.  In this case, there will be an association between $X$ and $Y$ even though there is no treatment effect.  Figure \@ref(fig:collider) is a common way to depict *selection bias* where an association in subjects selected for the study is not present in the general population.  In the selection bias diagram, $Z$ is an indicator of selection into the study with a box around it because researchers only observe subjects selected into the study.  

![(#fig:collider) Collider $Z$ with no treatment effect of $X$ on $Y$.](./images/collider.png)

A very famous example appears in @pearl2018book.  In the general population, we would not expect to see an association between an individual's talent and looks.  More talented people are not better looking and vice versa.  However, if we look only at famous Hollywood actors ($Z$), we would see an association between talent and looks in this group because an individual needs either talent or looks (or both) to become a famous Hollywood actor.  More seriously, *selection bias* is a huge issue in medical and public health studies resulting in easily misinterpretated associations [@hernan2004structural;@cole2010illustrating;@elwert2014endogenous].

### Mediator

Figure \@ref(fig:mediator) depicts mediator $M$ of the effect of treatment $X$ on outcome $Y$.  In this case, $X$ is a cause of $M$, $M$ is a cause of $Y$, and there is no effect of $X$ on $Y$ that cannot be explained by the effect of $M$ on $Y$.  A common mistake is to adjust for $M$.  If $M$ is categorical, we will not observe an association between $X$ and $Y$ within levels of $M$.  However, there is still an effect of $X$ on $Y$.  

![(#fig:mediator) Mediator $M$ of the effect of treament $X$ on outcome $Y$.](./images/mediator.png)

For example, let's say researchers are investigating an adverse reaction to a medication using a treatment group and a control group where the adverse reactions are always preceded by increased blood pressure.  Even if there is an effect of the medication on the adverse reason, when we condition upon experiencing increased blood pressure, we will not observe an association between the medication and adverse reactions.

Unfortunately, some researchers condition upon everything that they measure, often resulting in poor estimates of effects [@hernan2002causal].

<!--chapter:end:01-causality.Rmd-->

# Quantitative explanatory variable with categorical confounding variable

```{r, include = FALSE}
library(tidyverse)
```

In many situations we wish to examine the relationship between a response and more than one explanatory variable.  For instance, we might have a treatment variable (X) and a response variable (Y) but the relationship between them is confounded by a third variable (C).  Recall from chapter 1 that confounding occurs when factors determining assignment to treatment groups are also causes of the response; in other words, when there are one or more variables that affect to both our treatment variable and response variable.

For instance, suppose we want to investigate the relationship between ice cream sales and hospital admissions for heart attacks in the summer in NYC.  In this example, our "treatment" is volume of ice cream sales on any given day in NYC and our response is number of hospital admissions that day for heart attacks.   If we only look at those two variables, we might see a strong correlation; as ice cream sales increase, so do heart attacks.  But it would be a mistake to infer that eating ice cream causes heart attacks from this analysis alone.  There is a third variable that is related to both ice cream sales and heart attacks: temperature.  People tend to buy more ice cream when it is hot; if there also tend to be more heart attacks when it is hot, temperature confounds the relationship between ice cream sales and hospital admissions for heart attack. Recall the causal diagram from chapter 1. <!--![(#fig:confounder) A confounding variable $C$ on the effect of $X$ on $Y$.](./images/confounder.png)--> If we extend it to this example, we have

![Causal diagram for ice cream sales and heart attacks.](./images/ice_cream_graph.png)

Notice that we believe that a change in temperature *causes* a change in both ice cream sales and heart attacks.

## Unadjusted effect of $X$ on $Y$

Let’s review simple linear regression.  There exists a belief that height is protective against high blood pressure; that is, taller people tend to have lower blood pressure.  If we were interested in exploring this in a (relatively) representative sample of the adult U.S. population we might use data from the CDC’s National Health and Nutrition Examination Survey (NHANES).  The data used in this example is available on the course website.  It contains data sampled from 1999 to 2006 on 12,671 Americans age 18 and older who identify as either male or female and non-Hispanic white, non-Hispanic black or Mexican American, were not pregnant or taking high blood pressure medication.  We are interested in estimating the effect of height (HT) on mean systolic blood pressure (SBP). We could model this with the following regression equation.
$$\widehat{SBP}=\beta_0+\beta_1*HT$$

If we fit a regression model with height as the treatment variable and systolic blood pressure as the response, we get the following results:

```{r, message = FALSE}
bp_dat<-read_csv("./data/blood_pressure.csv")
ht_mod=lm(SBP~HT,
         data = bp_dat)
summary(ht_mod)
```

We note that the coefficient on height is actually positive, meaning that taller people actually tend to have higher blood pressure!  In fact, we expect SBP to go up by about 0.05 mmHG for every cm in additional height. This makes sense when we look at the data.   There is A LOT of data here, but we see a general increase in SBP as height increases.

```{r, fig.height = 3}
bp_dat%>%
  ggplot(aes(x=`HT`,
             y=`SBP`))+
  geom_point()+
  theme_classic()
```
  
This is contrary to what we expected.  So I guess previous research was wrong.  Wait – not so fast.  Gender is a potential confounder.  

## Effect of $X$ on $Y$ adjusting for $C$

Men tend to be taller than women (175 cm vs 162cm).  Men also tend to have higher mean systolic blood pressure (122 vs 118 mmHG).  Gender is a possible confounder.  Below is our data, colored by gender.

```{r, fig.height = 3}
bp_dat%>%
  ggplot(aes(x=`HT`,
             y=`SBP`))+
  geom_point(aes(colour = GENDER))+
  theme_classic()
```
  
Here we can see that it looks as if *within men* SBP tends to decrease as height increases.  Likewise *within women* we see the same. If we want to estimate the effect of height on blood pressure, we must first adjust for gender. We do this using something called Multiple Regression.  “Multiple” because we have multiple variables on the right hand side of the equation.  (This could be multiple explanatory variables or, as we have here a single explanatory variable with one or more confounding variables.)   Often we do this simply by adding the other variable to the regression equation.  But how do we add a categorical variable to a mathematical equation.  Suppose my regression model is 
  
$$\widehat{SBP}=\beta_0+\beta_1*HT+\beta_2*GENDER$$
    
What does it mean to multiply something by "male"?    When we want to include categoroical variables as explanatory variables in regression models, we often use indicator (allso called 'dummy') variables.  In this case since we are limiting our study population to only subjects who identify as male or female, our variable gender only has two levels.  We need one indicator variable to represent gender.  We will let our indicator variable equal 1 if a subject was male and 0 if the subject was female.  Then $\beta_2$ can be interpretated as the expected increase in SBP when a subject is male.
    
    
A possible multiple regression equation with SBP as our response, height as the treatment adjusting for gender as a confounder is

$$\widehat{SBP}=\beta_0+\beta_1*HT+\beta_2*GENDER$$
  
When we add gender to the model, we get the following results.

```{r}
ht_gen_mod=lm(SBP~HT+GENDER,
         data = bp_dat)
summary(ht_gen_mod)
```

We see that the relationship between height and SBP is now negative.  Specifically, for every extra centimeter of height, we expect SBP to go down by 0.24 mmHG.  Gender confounds the relationship between height and blood pressure.  We can also note that the coefficient for gender is a little over 9, meaning that on average, men have a SBP 9 mmHg higher than women.

Hmm, what are we forgetting…?  Model diagnostics!  

## Assessing model adequacy

We have just done a theory-based test but we haven’t confirmed if the validity conditions are met.  The validity conditions for multiple regression are analogous to those for simple regression.
1. Independence; once I have accounted for everything in the model, the responses can be considered independent of each other.

2. The model is linear; when plotting the residuals vs predicted values, there does not appear to be a pattern. 
3. The residuals have constant variance; when plotting the residuals vs predicted values, there is a constant width.

4. Normality; a histogram of the residuals is approximately normal.

In practice, it is difficult to check for independence in the data.  Unless there is a specific pattern to the way the data was collected, violations of this assumption may not be evident in our residual plots.  The best way to verify this assumption is to know how the data was collected.  Is it reasonable to consider it a random sample?  If, for instance, we had repeated measurements on the same people, this assumption may not be reasonable.  One way to check is to plot the residuals in the order they appear in your data.  But if the dependence is not related to the order the data was stored, issues may not be visible in this plot.

To check for linearity, we look at the residuals vs fitted values plot.  Here we are looking for any pattern in the residuals.  They should be scattered roughly evenly above and below the y=0 line across the entire graph.  (To check more rigorously in multiple regression we would actually check each explanatory variable individually using something called a partial residual plot or component plus residual plot.  That is beyond the scope of this course.)

To check for constant variance, we again look at the residuals vs predicted values.  We are checking to see if the variability in the residuals is approximately the same for different values of the response.  A widening of our residuals as y_hat increases, for instance, would indicate this assumption is not met.

Finally, to check for normality, we look at a histogram of the residuals to see if it looks at least approximately normal.


For our blood pressure example, we only have one observation on each person and we have a pseudo-random sample of the U.S. (Details on the exact sampling plan can be found on the CDC’s NHANES website.)   Additionally, if we plot the residuals vs their index (i.e. in the order they are in the  data) we see  no clear pattern to the residuals.   It seems reasonable that our independence validity condition is met.

```{r}
plot(1:length(ht_gen_mod$residuals),ht_gen_mod$residuals)
```


The plot of residuals vs fitted values from our model with both height and gender is in the following figure.  The plot does not show a pattern.  The residuals appear to be scattered roughly equally above and below the line y=0. It seems reasonable to believe that the linearity validity condition is met.

```{r}
plot(ht_gen_mod$fitted,ht_gen_mod$residuals)
```

From this same plot, we see that the variance of the residuals also seems to remain relatively constant as our predicted SBP increases.   The validity condition requiring equal variance seems reasonable.


The histogram of our residuals looks relatively normal although it does seem to have a heavy right tail.   Our residuals may not be strictly normal.  However, linear regression is “robust” to departures from normality.  That means that slight, or even moderate, departures from normality generally do not pose a problem.  Our residuals look “normal enough” that I feel comfortable concluding that our validity conditions are met.

```{r}
hist(ht_gen_mod$resid)
```

What would we expect to see if the validity conditions were violated?  Below are some examples of model diagnostic plots that indicate a departure from our validity conditions.  We simulated the data used to create these plots so we know which conditions aren’t valid.  These are also relatively extreme examples for illustrative purposes; in real data you often won’t see patterns this obvious.  Violations of our model’s validity conditions can be especially hard to detect when we have a small sample size.

First we look at independence.  The following plot of residuals in order show a clear pattern.  The residuals seem to be in triplets; first we have a positive residual, then a residual around zero, then a negative residual.  This pattern indiactes that are model is missing an important factor, like time.  Perhaps the data was a reading collected three times a day - morning, noon and night - and we expect the reading will differ based on time of day but we did not include that in the model.  Departures from independence are rarely this easy to spot.

```{r, echo=FALSE}
x=runif(33,10,20)
x2=rep(c(3,0,-3),11)
y=2*x+10*x2+rnorm(33,10,3)
m1=lm(y~x)
plot(1:33,m1$residuals)
```

<!--When we include the *time* variable in the model, we see that the pattern disappears. [I think maybe I leave this part out?]

```{r, echo=FALSE}
m1=lm(y~x+x2)
plot(1:33,m1$residuals)
```
-->

Next we look at the linearity assumption.  Below we see a residuals vs fitted values plot that shows a clear pattern.  Fitted values below about 150 or above about 350 tend to have positive residuals (i.e. be underestimated) whereas fitted values between 150 and 350 tend to have negative residuals (i.e. be overestimated).  This is an indication that our lineariyt assumption is violated; the right hand side of the model is not correctly specified.

```{r, echo=FALSE}
x=runif(50,10,20)
y2=x+x^2+rnorm(50,10,3)
m2=lm(y2~x)
plot(m2$fitted.values,m2$residuals)
```

<!--In fact, here $Y$ was a function of $X^2$, not $X$.  When we add $X^2$ to the model, we again see that the probelm disappears.

```{r, echo=FALSE}
m2=lm(y2~x+I(x^2))
plot(m2$fitted.values,m2$residuals)
```
-->

Next we investigate our assumption of constant variance.  In the plot below we see a clear fanning out of the residuals; as $\hat{y}$ increases, so does the variance of the residuals.

```{r, echo=FALSE}
x=sort(runif(500,10,20))
y3=x+rnorm(500,rep(0,500),1:500)
m3=lm(y3~x)
plot(m3$fitted.values,m3$residuals)
```

And finally, we investigate our normality assumption.  In the plot below, we see that are residuals are heavily skewed to the right.  The assumption ofnormality is violated.

```{r echo=FALSE}
x=runif(500,10,20)
y4=x+rexp(500,.1)
m4=lm(y4~x)
hist(m4$residuals)
```

In our example, researchers were specifically interested in the relationship between height and blood pressure so height was our treatment variable.  Gender was related to both height and blood pressure so we need to include it in our model, but we are not particularly interested in the relationship between gender and blood pressure from a clinical or scientific standpoint.  So gender was a confounding variable.  Notice that we treat these variables the same from a statistical standpoint.  It is possible to have multiple exposures or treatments, multiple confounders, or both.  The distinction between the two is not a statistical one; it is a scientific one.  
<!--[Note about prediction.]-->

<!--chapter:end:03-categorical.Rmd-->

# Activity {#activity}

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE,
                      message = FALSE,
                      warning = FALSE)
library(tidyverse)
library(knitr)
library(broom)
```

*Title*: The Indoor Obstacle Course Test (IOCT)

*Topics*}*: Confounding, Causal Diagrams, Simple Linear Regression, Confidence Intervals

*Background*: Cadets at West Point must pass the [Indoor Obstacle Course Test](https://www.youtube.com/watch?v=94tPO0fGtJo&t=77s) (IOCT) to graduate.  The IOCT begins with a series of floor and climbing obstacles and ends with several laps around an indoor track.  It is an exhausting test of endurance and strength. In addition to being a graduation requirement, cadets receive a letter grade that is factored into their class rank.

![Crawl Obstacle](./images/crawlObstacle.jpg){width=50%} ![Tire Obstacle](./images/tireObstacle.jpg){width=50%}

Shorter cadets often argue they are at a disadvantage on the obstacle course.  Many obstacles appear to favor taller cadets because they are easier to reach.  In this study, we will investigate the effect of height on IOCT times.

1. [Watch the video of Cadet Madaline Kenyon running the IOCT](https://www.youtube.com/watch?v=94tPO0fGtJo&t=77s). In your opinion, do some obstacles favor taller cadets? Explain.

\vspace{1in} 

The file \texttt{obstacle\_course.csv} contains height (inches), IOCT times (seconds), biological sex (M/F), and whether the cadet played an intercollegiate sport for a sample of 384 cadets who ran the IOCT course in the last five years.  

2. What is the explanatory variable in this study? Classify the variable as quantitative or categorical.

\vspace{0.25in}

3. What is the response variable in this study? Classify the variable as quantitative or categorical.

\vspace{0.25in}

4. Is this study an observational study or a randomized experiment? Explain.

\vspace{1in}

\newpage

Figure 1 depicts IOCT times in seconds versus height in inches.  Table 1 contains information from the linear regression model.

```{r, fig.cap = "Indoor Obstacle Course Test (IOCT) times versus height (n = 384)", fig.height=3.5}
cadets <- read_csv(file = "./data/obstacle_course.csv")

cadets %>% 
  filter(IOCT_Time < 400) %>% 
  ggplot(aes(x = height, y = IOCT_Time)) +
  geom_point() +
  geom_smooth(method = "lm", se = FALSE) +
  ylim(0,400) +
  theme_bw() +
  labs(y = "IOCT time (sec)", x = "height (in)")

cadets %>% 
  filter(IOCT_Time < 400) %>%
  lm(IOCT_Time ~ height, data = .) %>% 
  tidy() %>% 
  kable(digits = 2, caption = "Linear regression output for IOCT times and height.")
```

5. Interpret the estimate of the height coefficient in Table 1. 

    \vspace{1in}

6. Calculate and interpret a 95\% confidence interval for the slope coefficient.

\vspace{1in}

\newpage

7. The $p$-value for height in Table 1 indicates there is strong evidence of an association between height and IOCT time.  Taller cadets tend to do better on the IOCT.   Some people would say the result is *statistically significant*.  However, statistical significance and practical signifigance are different.  [Review the grade scale for the IOCT.](https://en.wikipedia.org/wiki/Indoor_Obstacle_Course_Test)  In your opinion, does the observed association have practical significance? Explain.

\vspace{1in}

8. A shorter cadet argues Figure 1 shows evidence the IOCT is unfair based on height.  Do you agree or disagree? Explain.

\vspace{1in}

9. Briefly explain the difference between these two conclusions.

* *Height is associated with faster IOCT times.*

* *Height causes faster IOCT times.*

\vspace{1in}

10.  Based on the analysis presented thus far, is it possible to distinguish between these two explanations? Explain.

\vspace{1in}

11. Draw a causal diagram depicting the relationship between height, IOCT time, and sex.  Explain your decisions to include/exclude arrows in the diagram.

\vspace{1in}

12. Based on your diagram, identify the confounding variable.

\vspace{0.5in}

Below are boxplots of height in inches and IOCT times in seconds by sex.

```{r, fig.cap="Height (inches) and IOCT time (seconds) by sex.", fig.height=3}
cadets %>%
  filter(IOCT_Time < 400) %>%
  pivot_longer(cols = c(-sex,-athlete),
               names_to = "variable",
               values_to = "value") %>% 
  ggplot(aes(x = sex, y = value)) +
  geom_boxplot() +
  facet_wrap(~variable, scale = "free_y") +
  labs(y = "", x = "")
```

13. Based on Figure 2, is the estimate of the effect of height on IOCT time in Table 1 confounded by sex? If so, is the effect of height smaller or larger than that reported in Table 1? Explain.

\vspace{1in}

\newpage

Figure 3 depicts the association between IOCT time and height by sex. Tables 2 and 3 depict regression results for female and male cadets, respectively.

```{r,fig.cap="Indoor Obstacle Course Test (IOCT) times versus height by sex (n = 384).", fig.height=3}
cadets %>%
  filter(IOCT_Time < 400) %>%
  ggplot(aes(x = height, y = IOCT_Time, color = sex)) +
  geom_point() +
  geom_smooth(method = "lm", se = FALSE) +
  labs(y = "IOCT time (sec)", x = "height (in)")
```

```{r}
cadets %>%
  filter(IOCT_Time < 400, sex == "F") %>%
  lm(IOCT_Time ~ height, data = .) %>% 
  tidy() %>% 
  kable(digits = 2, caption = "Regression results for female cadets.")

cadets %>%
  filter(IOCT_Time < 400, sex == "M") %>%
  lm(IOCT_Time ~ height, data = .) %>% 
  tidy() %>% 
  kable(digits = 2, caption = "Regression results for male cadets.")
```

14. Based on Figure 3 and Tables 2 and 3, does it appear there is an association between IOCT time and height within levels of sex? Explain.

\vspace{1in}

\newpage

15. In your opinion, is there much evidence that height is an advantage on the IOCT (in other words, is height the *cause* of better IOCT times)?  Explain.

\vspace{2in}

16.  Briefly discuss two ways you could improve this study to better assess whether there is a height advantage.

\vspace{1in}


<!--chapter:end:05-activity.Rmd-->

`r if (knitr::is_html_output()) '
# References {-}
'`

<!--chapter:end:07-references.Rmd-->

