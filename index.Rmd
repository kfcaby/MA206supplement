--- 
title: "Causality and Multiple Regression Supplement - MA206 Probability and Statistics"
author: "Authors TBD"
date: "`r Sys.Date()`"
site: bookdown::bookdown_site
documentclass: book
bibliography: [book.bib, packages.bib]
biblio-style: apalike
link-citations: yes
description: "This is a supplement for MA206 Probability and Statistics, West Point, NY."
---

# (Kevin) Motivation

## Goals of quantitative research (describe, predict, cause-and-effect)

Researchers and practitioners in nearly every discipline employ quantitative methods to answer important questions and make decisions.  Regardless if it is finance, medicine, psychology, etc., all researchers typically have one of three goals [@cozby2020methods] when they conduct quantitative research:

* **describe**

* **predict** 

* **cause-and-effect**

(discuss external and internal validity here?)

For example, consider medical researchers investigating a newly discovered variant of the coronavirus.  Initially, they focus on describe studies.  *How prevalent is the variant in the population? How does the prevalence vary over space and time? What groups of individuals are most vulnerable to the variant? What else is associated with the variant?*   Together, these studies help the researchers better understand the variant and identify important variables for refined predict studies.  

Predict studies identify individuals most at risk for the variant or for transmitting it to others. Researchers select potential predictors as inputs to algorithms and statistical methods that transform them into estimates of each individual's risk for the variant.  These methods rely upon associations between the inputs and the variant.  In addition, these methods estimate the error associated with their predictions. Researchers try to ensure error estimates obtained within their study are good estimates of the error when their methods are applied to other populations.  While predict studies help us identify who is at risk, they do not tell us the effect on their risk if we change something about them (quitting smoking, lost weight, etc.)  To answer these "what if?" questions, researchers use cause-and-effect studies.

Cause-and-effect studies try to estimate the effect of intervening or changing some aspect of an individual.  These studies are tremendously important in public policy (where they are aptly called *intervention studies*) to justify expensive legislation and regulations to reduce risk.  Unlike prediction studies, which are highly algorithmic and only require researchers to identify appropriate inputs, cause-and-effect studies use subject matter knowledge of the causal structure of the inputs to select appropriate statistical models [@hernan2019second].  For example, to assess the affect of quitting smoking on coronavirus risk, we would have to understand why some people quit smoking and others do not, as these reasons themselves may be responsible for some of the reduction in risk we observe between the two groups. Also, unlike prediction studies which rely upon what we *see*, cause-and-effect studies require us to ask what would happen if we *do* something [@pearl2018book].  Only human beings are capable of such counterfactual thinking ("what would have happened if") - even our most sophisticated computer algorithms require human supervision to address cause-and-effect questions.

**Exercises**

1. For the following studies, identify the goal (describe, predict, cause-and-effect) of the study.  Briefly explain your choice:

* *A researcher obtains school records to collect information on the extracurricular activities of students.*

* *A researcher obtains school records to determine the impact of extracurricular activities on grades.*

* *A researcher obtains school records on extracurricular activities to identify students who are at risk to graduate.*

2.  Select an area of interest to you.  Write a sentence describing a study in your area of interest for each of the three goals (describe, predict, cause-and-effect)

## Why multiple regression?

(need to edit the below from an email)

You are investigating the relationship between education and adult earnings.  (Note this is almost always an observational study -- a survey of sampled individuals with variables such as income, education, age, sex, etc.)  

 

Let's assume you've never taken our course (or one like it).  You can get pretty far investigating this data with concepts most students learn in high school.  For example, you could compare adult earnings in college graduates to those of non-college graduates or find an estimate of earnings per year of education.  If you are especially savy, you might recognize that college graduates and noncollege graduates are different in ways (age, sex, etc.) that are also important to adult earnings and repeat your original analyses within the different levels of these variables.  To you (and most people outside our discipline), statistics is arithmetic, and it is hard to envision anything beyond arithmetic if that's all you know. However, there are important questions you cannot answer with this limited view of statistics: 

 

(1) What range of population values for measures of association between education and adult earnings are compatible with your data?    

(2) If there is no association between education and earnings, how extreme are your observed results?

(3) What is the association between education and earnings holding other important variables constant?

(4) How does the relationship between education and earnings depend upon other variables?

 

Most people's cynicism towards statistics is rooted in a lack of understanding that our discipline has answers to (1)-(4).

 

Thus far in our course, we have focused on (1) and (2) for various types of independent and dependent variables.  And, if someone were to ask me, "what have your students learned in your course this semester?", my response would start with (1) and (2).  These are important concepts...humans are not predisposed to think this way. 

 

However, you still cannot answer (3) and (4) if you're in our course right now.  (3) relates to confounding; (4) is interactions. The concepts of confounding and interactions are best understood before approaching multiple regression (a point we discuss at length here: https://www.overleaf.com/read/nkzpvpxwsjzq).  However, multiple regression is the most flexible statistical model for estimating quantities in (3) and (4).  Being able to answer (3) and (4) is a huge step up in a student's capability.  Comparing earnings of college graduates to noncollege graduates is meaningless if these groups are vastly different with regards to age, sex, etc.  Unless you have formal education in these methods, you probably don't know (3) and (4) are possible -- it's outside what you can do with arithmetic. 

 

Okay, so that's some discussion on why do we teach multiple regression?  Now, what do we teach about multiple regression?

 

Broadly speaking, there are two major aspects to multiple regression: theory, application.  Given our students and where this course fits into their education, I am an unabashed proponent of application being the primary focus of our course.

 

Examples of theory - derive the least squares estimates of the regression coefficients, understand the geometric interpretation of regression, etc.

 

Examples of application - fit and interpret a model to estimate the association between education and earnings that adjusts for age.  

 

You can have a rich understanding of one of these two aspects and be a complete novice about the other.  From my experience, mathematicians are rock stars in theory but haven't seen as much application (don't worry, you have more than enough for this course).  For example, in application, a statistician would employ multiple regression in very different ways depending on whether the primary goal of the study is (a) prediction or (b) estimating the effect of an intervention.  In (a), the primary concern is making smart use of the data to obtain "good" estimates of the out-of-sample prediction error with little care to what's in the model.  In (b), the primary concern is confounding, which is a type of statistical bias, so variables are selected carefully by the researcher to reflect the intervention of interest.  The distinction between (a) and (b) is beyond the scope of our course, but I'm just trying to illustrate the richness there is to both the theory and application of multiple regression.

 So, back to our course, I think the Michigan house prices example in Intermediate Statistical Analyses by Tintle et. al. and Dan Baller's worksheet sums up well what I think students should get from our course.  Specifically, they should get an appreciation that there are statistical models that (1) estimate measures of association while holding other variables constant and (2) see how these associations change based on other variables.  For (2), an interaction between a quantitative and categorical variable is about as far as we can reasonably expect most students to get.




```{r include=FALSE}
# automatically create a bib database for R packages
knitr::write_bib(c(
  .packages(), 'bookdown', 'knitr', 'rmarkdown'
), 'packages.bib')
```